{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOh/JN8kjuu/NLf/Sxjkijs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkro/pytorch_for_deep_learning/blob/main/00_pytorch_fundamentals_extra_curriculum_pythorch_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n6NuqquUM5z8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# also available: TorchText, TorchAudio including datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
        "\n",
        "https://www.kaggle.com/datasets/zalando-research/fashionmnist"
      ],
      "metadata": {
        "id": "ApRxjIrnbS6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download training data from open datasets\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# download test data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuNuYZgONpLf",
        "outputId": "3f13514f-0fb0-434b-edf7-6906b7343c7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:00<00:00, 116318614.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 10061352.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 4422102/4422102 [00:00<00:00, 62468265.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18423444.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "#for X, y in test_dataloader:\n",
        "  #print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "  #print(f\"Shape of y: {y.shape} {y.dtype}\")\n"
      ],
      "metadata": {
        "id": "F_l82mhAOCNI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ5RyMxSO1IL",
        "outputId": "0c7b3821-29ec-4cb3-f097-89486cbcd847"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class for our neural network model\n",
        "class NeuralNetwork(nn.Module):  # Subclass PyTorch's base class for all neural network modules\n",
        "    def __init__(self):  # This method initializes the neural network\n",
        "        super().__init__()  # Call the initialization method of the parent class\n",
        "        self.flatten = nn.Flatten()  # An operation that flattens the input, i.e., it makes it one-dimensional\n",
        "\n",
        "        # A sequential container where we stack our layers\n",
        "        # (Linear layers with ReLU activation function)\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            # 28 * 28 coresponds to the pixel size of the mnist data items; 784 pixels = 784 \"features\"\n",
        "            nn.Linear(28*28, 512),  # A linear layer (also known as a fully connected layer), it takes a 1D tensor of size 28*28 and outputs a 1D tensor of size 512\n",
        "            nn.ReLU(),  # An activation function that introduces non-linearity in the model. It replaces negative pixel values in the previous layer with zero\n",
        "\n",
        "            # a \"hidden layer\", optional; we can have more of these too, but not necessarily for better results\n",
        "            # \"hidden\" means just that it's not directly connected to the input or output\n",
        "            nn.Linear(512, 512),  # Another linear layer that takes a 1D tensor of size 512 and outputs a 1D tensor of size 512\n",
        "            nn.ReLU(),  # Another ReLU activation layer\n",
        "\n",
        "            nn.Linear(512, 10),  # A final linear layer that takes a 1D tensor of size 512 and outputs a 1D tensor of size 10 (this size usually corresponds to the number of classes in a classification problem)\n",
        "            nn.ReLU(),  # A final ReLU activation layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # The method that defines the forward pass of the neural network\n",
        "        x = self.flatten(x)  # First, we flatten the input\n",
        "        logits = self.linear_relu_stack(x)  # Then, we pass the flattened input through the stack of layers\n",
        "        return logits  # We return the output of the network, also known as the logits\n",
        "\n",
        "# After defining the class for the network, we create an instance of the network\n",
        "model = NeuralNetwork()\n",
        "\n",
        "# 'device' is not defined in the code you provided, but it usually refers to where you want to run your code,\n",
        "# either on a CPU or a GPU. '.to(device)' moves the model parameters to the specified device.\n",
        "model = model.to(device)\n",
        "\n",
        "# Finally, we print the structure of the neural network.\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZzwhEPrO9vh",
        "outputId": "57571506-8810-438a-fba1-4210748e494e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For training a model, we need a loss function and an optimizer.\n",
        "\n",
        "# The loss function calculates how far the model's predictions are from the true values.\n",
        "# Here we're using CrossEntropyLoss, which is commonly used for multi-class classification problems.\n",
        "# It combines softmax and negative log likelihood loss in one single class.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# The optimizer is an algorithm or method used to adjust the attributes of your neural network\n",
        "# such as weights and learning rate in order to reduce the losses.\n",
        "# Optimizers help to get results faster.\n",
        "# Here we're using Stochastic Gradient Descent (SGD) as the optimizer.\n",
        "# Learning rate (lr) is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient.\n",
        "# The lower the value, the slower we travel along the downward slope.\n",
        "# While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima,\n",
        "# it could also mean that we'll be taking a long time to converge — especially if we get stuck on a plateau region.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "RfkJ1vFVRvHw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following function, the training process for a neural network model is defined, which includes the forward pass (making predictions), calculating the loss, performing backpropagation to compute gradients, and updating the model's parameters (weights and biases) using an optimization algorithm. The function also prints the training loss every 100 batches, which gives you a way to monitor the training process."
      ],
      "metadata": {
        "id": "2X0tIIQ-UNHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)  # Get the total size of the dataset\n",
        "    model.train()  # Set the model to training mode. This has any effect only on certain modules like Dropout or BatchNorm.\n",
        "\n",
        "    # Enumerate through the dataloader. In each loop, the data (X) and target labels (y) for a batch is returned.\n",
        "\n",
        "    # In each iteration of training:\n",
        "\n",
        "    # The network makes predictions on the input data (forward pass).\n",
        "    # The error between the network's predictions and the known correct outcomes (the \"labels\") is calculated using a loss function.\n",
        "    # The backpropagation algorithm then works out the contribution of each weight and bias to this error by calculating the gradients of the loss function with respect to each weight and bias.\n",
        "    # These gradients are then used to adjust the weights and biases to minimize the error (this is an optimization step, typically done using an algorithm like stochastic gradient descent).\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)  # Move the data and labels to the device where the model is. This is often a GPU, but could also be a CPU.\n",
        "\n",
        "        # Compute the prediction error. First, we run the forward pass to get the model's predictions.\n",
        "        pred = model(X)\n",
        "        # Then, we pass the model's predictions along with the true labels to the loss function, which computes the error.\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation. This involves three steps:\n",
        "        # 1. First, we calculate the gradients of the loss with respect to the model's parameters. This is done using the backward() function.\n",
        "        loss.backward()\n",
        "        # 2. Then, we perform a single optimization step (parameter update) using the optimizer. This adjusts the model's parameters based on the gradients computed in the backward step.\n",
        "        optimizer.step()\n",
        "        # 3. Finally, we zero out the gradients. This is because by default, gradients are accumulated in buffers, so we need to manually set them to zero before the next iteration.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print the current loss and training progress information every 100 batches.\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)  # Compute the current loss and the number of examples seen so far.\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")  # Print the current loss and training progress.\n"
      ],
      "metadata": {
        "id": "s8X7ZyZNSnm3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice chatgpt analogy for gradient descent and backpropagation:\n",
        "\n",
        ">Backpropagation and the concept of gradients can be complex topics, especially if you're just getting started with neural networks and deep learning. It's great that you're asking questions and seeking to understand these concepts.\n",
        ">\n",
        ">Here's a simpler way to think about it: imagine you're on a hill and your goal is to get to the bottom. You're blindfolded, so you can't see where to go, but you can feel the slope of the hill under your feet.\n",
        ">\n",
        ">If you feel the ground sloping downwards to your right, you know that if you step to the right, you'll go downhill. If you feel the ground sloping downwards to your left, you know that if you step to the left, you'll go downhill. By repeatedly feeling the slope under your feet and taking a step downhill, you'll eventually reach the bottom of the hill.\n",
        ">\n",
        ">In this analogy, the hill is the loss function, your position on the hill is the weights of the neural network, and the slope that you feel under your feet is the gradient. Just like how you can use the slope to figure out which way to step to go downhill, a neural network can use the gradient to figure out how to adjust its weights to reduce the loss.\n",
        ">\n",
        ">The process of calculating the slope (gradient) is what we call backpropagation, and the process of taking a step downhill (updating the weights) is what we call an optimization step."
      ],
      "metadata": {
        "id": "u3aQEmjvWidY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the models performance against the test dataset\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    total_samples = len(dataloader.dataset)  # Total number of samples in the dataset\n",
        "    num_batches = len(dataloader)  # Total number of batches in the dataloader\n",
        "    model.eval()  # Set the model to evaluation mode. This has any effect only on certain modules like Dropout or BatchNorm.\n",
        "\n",
        "    # Initialize counters for total test loss and number of correct predictions\n",
        "    test_loss, correct_predictions = 0, 0\n",
        "\n",
        "    # torch.no_grad() informs PyTorch that we do not want to perform back-propagation, which reduces memory usage and speeds up computation.\n",
        "    with torch.no_grad():\n",
        "        # Loop over each batch from the testing data\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)  # Move the data and labels to the device where the model is.\n",
        "\n",
        "            # Make predictions using the model\n",
        "            predictions = model(X)\n",
        "\n",
        "            # Compute the loss between the predictions and the true labels, and add it to the total test loss\n",
        "            test_loss += loss_fn(predictions, y).item()\n",
        "\n",
        "            # For each prediction, get the index of the label with the highest predicted probability using argmax,\n",
        "            # then compare with the true label. This gives us a binary vector where 1 represents a correct prediction and 0 represents a wrong prediction.\n",
        "            correct_predictions_vector = (predictions.argmax(1) == y)\n",
        "\n",
        "            # Sum up the correct predictions, convert to float (for division later), and add to the total number of correct predictions\n",
        "            correct_predictions += correct_predictions_vector.type(torch.float).sum().item()\n",
        "\n",
        "    # Calculate average loss over all batches\n",
        "    average_test_loss = test_loss / num_batches\n",
        "\n",
        "    # Calculate the accuracy as the proportion of correct predictions over total samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # Print the test error details\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {average_test_loss:>8f} \\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ROZQdOsDW6cJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test the model\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n--------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdBxO2HGaJQ2",
        "outputId": "ab96fdd6-c1a1-43f6-b429-ac8eddd4766a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "--------------------------------\n",
            "loss: 1.441873  [   64/60000]\n",
            "loss: 1.419557  [ 6464/60000]\n",
            "loss: 1.329365  [12864/60000]\n",
            "loss: 1.445377  [19264/60000]\n",
            "loss: 1.239895  [25664/60000]\n",
            "loss: 1.445290  [32064/60000]\n",
            "loss: 1.226425  [38464/60000]\n",
            "loss: 1.262740  [44864/60000]\n",
            "loss: 1.358987  [51264/60000]\n",
            "loss: 1.348363  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.5%, Avg loss: 1.287357 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------------\n",
            "loss: 1.289905  [   64/60000]\n",
            "loss: 1.286245  [ 6464/60000]\n",
            "loss: 1.188234  [12864/60000]\n",
            "loss: 1.333966  [19264/60000]\n",
            "loss: 1.121187  [25664/60000]\n",
            "loss: 1.345068  [32064/60000]\n",
            "loss: 1.115078  [38464/60000]\n",
            "loss: 1.170060  [44864/60000]\n",
            "loss: 1.263788  [51264/60000]\n",
            "loss: 1.266954  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.7%, Avg loss: 1.197084 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------------\n",
            "loss: 1.189699  [   64/60000]\n",
            "loss: 1.198598  [ 6464/60000]\n",
            "loss: 1.091549  [12864/60000]\n",
            "loss: 1.257800  [19264/60000]\n",
            "loss: 1.044701  [25664/60000]\n",
            "loss: 1.271755  [32064/60000]\n",
            "loss: 1.043425  [38464/60000]\n",
            "loss: 1.110263  [44864/60000]\n",
            "loss: 1.194313  [51264/60000]\n",
            "loss: 1.208741  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.9%, Avg loss: 1.133679 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------------\n",
            "loss: 1.118648  [   64/60000]\n",
            "loss: 1.136937  [ 6464/60000]\n",
            "loss: 1.021294  [12864/60000]\n",
            "loss: 1.203880  [19264/60000]\n",
            "loss: 0.992760  [25664/60000]\n",
            "loss: 1.216687  [32064/60000]\n",
            "loss: 0.993600  [38464/60000]\n",
            "loss: 1.069856  [44864/60000]\n",
            "loss: 1.142577  [51264/60000]\n",
            "loss: 1.165066  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.8%, Avg loss: 1.086952 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------------\n",
            "loss: 1.064840  [   64/60000]\n",
            "loss: 1.090115  [ 6464/60000]\n",
            "loss: 0.967866  [12864/60000]\n",
            "loss: 1.164122  [19264/60000]\n",
            "loss: 0.955886  [25664/60000]\n",
            "loss: 1.174262  [32064/60000]\n",
            "loss: 0.956743  [38464/60000]\n",
            "loss: 1.041429  [44864/60000]\n",
            "loss: 1.102553  [51264/60000]\n",
            "loss: 1.130078  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.9%, Avg loss: 1.051023 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjoBtgVja7wi",
        "outputId": "ef531d31-17b2-4147-9ce1-95adb67d3331"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a model\n",
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcZblLcFa_2n",
        "outputId": "4eb9274c-ae70-4aaf-d204-acc893a3bd9a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a prediction on the first item of the fashionmnist test set (a grayscale image of 28*28 pixel of an ankle boot)\n",
        "\n",
        "# Define names of the classes (labels) corresponding to the numerical predictions 0-9 so we don't just see \"5\" but \"Sandal\" in the prediction\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f__mNM2bgh9",
        "outputId": "a623a140-ca8a-4e48-f53a-794504fd2097"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    }
  ]
}